#!/bin/bash
# Script de backup automatique pour Cureliah

set -e

# Variables d'environnement
PROJECT_ID=${SUPABASE_PROJECT_ID:-"your-project-id"}
ACCESS_TOKEN=${SUPABASE_ACCESS_TOKEN:-"your-access-token"}
S3_BUCKET=${BACKUP_S3_BUCKET:-"cureliah-backups"}
BACKUP_DIR="/backups"
DATE=$(date +%Y-%m-%d-%H-%M-%S)

echo "üîÑ D√©marrage du backup automatique - $DATE"

# Cr√©er le dossier de backup
mkdir -p "$BACKUP_DIR/$DATE"

# 1. Backup de la base de donn√©es Supabase
echo "üì¶ Backup de la base de donn√©es..."
if command -v supabase &> /dev/null; then
    # Export de la structure et des donn√©es
    supabase db dump --project-id "$PROJECT_ID" > "$BACKUP_DIR/$DATE/database.sql"
    
    # Export des policies RLS
    supabase db dump --project-id "$PROJECT_ID" --schema auth > "$BACKUP_DIR/$DATE/auth_policies.sql"
    
    echo "‚úÖ Backup base de donn√©es termin√©"
else
    echo "‚ö†Ô∏è Supabase CLI non disponible, utilisation de pg_dump..."
    # Fallback avec pg_dump si Supabase CLI n'est pas disponible
    # pg_dump $DATABASE_URL > "$BACKUP_DIR/$DATE/database.sql"
fi

# 2. Backup des fichiers de storage
echo "üìÅ Backup des fichiers de storage..."
# TODO: Implement storage backup logic
# supabase storage download --project-id "$PROJECT_ID" --bucket-name "avatars" "$BACKUP_DIR/$DATE/storage/"

# 3. Backup des configurations
echo "‚öôÔ∏è Backup des configurations..."
cp -r /app/config "$BACKUP_DIR/$DATE/" 2>/dev/null || echo "Pas de dossier config √† sauvegarder"

# 4. Cr√©er l'archive
echo "üóúÔ∏è Cr√©ation de l'archive..."
cd "$BACKUP_DIR"
tar -czf "cureliah-backup-$DATE.tar.gz" "$DATE/"

# 5. Upload vers S3
echo "‚òÅÔ∏è Upload vers S3..."
if command -v aws &> /dev/null; then
    aws s3 cp "cureliah-backup-$DATE.tar.gz" "s3://$S3_BUCKET/daily-backups/"
    echo "‚úÖ Backup upload√© vers S3"
else
    echo "‚ö†Ô∏è AWS CLI non disponible, backup local uniquement"
fi

# 6. Nettoyage des anciens backups (garder 7 jours)
echo "üßπ Nettoyage des anciens backups..."
find "$BACKUP_DIR" -type f -name "cureliah-backup-*.tar.gz" -mtime +7 -delete
rm -rf "$BACKUP_DIR/$DATE"

# 7. Nettoyage S3 (garder 30 jours)
if command -v aws &> /dev/null; then
    aws s3api list-objects-v2 --bucket "$S3_BUCKET" --prefix "daily-backups/" --query "Contents[?LastModified<='$(date -d '30 days ago' --iso-8601)'].Key" --output text | xargs -r -I {} aws s3 rm "s3://$S3_BUCKET/{}"
fi

# 8. V√©rification de l'int√©grit√©
echo "üîç V√©rification de l'int√©grit√©..."
if [ -f "cureliah-backup-$DATE.tar.gz" ]; then
    BACKUP_SIZE=$(stat -f%z "cureliah-backup-$DATE.tar.gz" 2>/dev/null || stat -c%s "cureliah-backup-$DATE.tar.gz")
    if [ "$BACKUP_SIZE" -gt 1000 ]; then
        echo "‚úÖ Backup cr√©√© avec succ√®s ($BACKUP_SIZE bytes)"
        
        # Log du succ√®s
        echo "$(date): Backup successful - Size: $BACKUP_SIZE bytes" >> /var/log/backup.log
        
        # Notification de succ√®s (optionnel)
        if [ -n "$SLACK_WEBHOOK" ]; then
            curl -X POST -H 'Content-type: application/json' \
                --data "{\"text\":\"‚úÖ Backup Cureliah r√©ussi - $DATE ($BACKUP_SIZE bytes)\"}" \
                "$SLACK_WEBHOOK"
        fi
    else
        echo "‚ùå Erreur: Backup trop petit"
        exit 1
    fi
else
    echo "‚ùå Erreur: Fichier de backup non cr√©√©"
    exit 1
fi

echo "üéâ Backup termin√© avec succ√®s!"
